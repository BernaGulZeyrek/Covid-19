{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anlık tweetlerin elde edilmesi\n",
    "import base64\n",
    "import datetime\n",
    "\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "import time\n",
    "import json\n",
    "\n",
    "consumer_key=\"\"\n",
    "consumer_secret=\"\"\n",
    "access_key=\"\"\n",
    "access_secret=\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file = open(r\"C:\\Users\\ASUS\\Desktop\\corona3.txt\",'a',encoding=\"utf-8\")\n",
    "\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    batch_size=50\n",
    "    tweets=[]\n",
    "    tweet_batch_size=[]\n",
    "    \n",
    " \n",
    "    def on_status(self, status):\n",
    "        \n",
    "            \n",
    "        #s1 = json.dumps(status)\n",
    "        #tweet_data = json.loads(s1)\n",
    "        tweet_data = json.dumps(status._json)\n",
    "        tweet_data=json.loads(tweet_data)\n",
    "        if \"extended_tweet\" in tweet_data:\n",
    "            tweet = tweet_data['extended_tweet']['full_text']\n",
    "            print(tweet)\n",
    "            if 'RT' not in tweet:\n",
    "                file.write(tweet+\"\\n\")\n",
    "                \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "\n",
    "    # complete authorization and initialize API endpoint\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth,parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "    streamListener = StreamListener()\n",
    "    stream = tweepy.Stream(auth=api.auth, listener=streamListener,tweet_mode='extended')\n",
    "\n",
    "    tags = ['stayhome','stay home','covid',\"corona\",\"coronavirus\",\"korona\",\"covid-19\",\"Covid19\",\"Covid-19\",\"Corona Virus\",\"pandemic\",\"pandemi\",\"COVID-19\"]\n",
    "    stream.filter(track=tags,languages = [\"tr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eski tweetlere ulşmak için bu kod eklenebilir\n",
    "import GetOldTweets3 as got\n",
    "text_query = 'pandemi'\n",
    "since_date = '2020-01-01'\n",
    "until_date = '2020-04-30'\n",
    "count = 10000\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setSince(since_date).setUntil(until_date).setMaxTweets(count).setLang('tr')\n",
    "tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "text_tweets = [[tweet.date, tweet.text] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duygusınıfları için \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data=pd.read_excel(r'C:\\Users\\ASUS\\Desktop\\duyguetiketi.xlsx')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sentiment\"].replace(1, value = \"pozitif\", inplace = True)\n",
    "data[\"sentiment\"].replace(-1, value = \"negatif\", inplace = True)\n",
    "data[\"sentiment\"].replace(0, value = \"nötr\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Counter(data['sentiment']).keys()\n",
    "sum_ = Counter(data['sentiment']).values()\n",
    "df = pd.DataFrame(zip(labels,sum_), columns = ['sentiment', 'Toplam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etiketlerin görselleştirilmesi - çubuk grafiği\n",
    "df.plot(x = 'sentiment' , y = 'Toplam',kind = 'bar', legend = False, grid = True, figsize = (15,5))\n",
    "plt.title('Kategori Sayılarının Görselleştirilmesi', fontsize = 20)\n",
    "plt.xlabel('Kategoriler', fontsize = 15)\n",
    "plt.ylabel('Toplam', fontsize = 15);\n",
    "#etiketlerin görselleştirilmesi - pasta grafiği\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.pie(df.Toplam, labels =df.sentiment, autopct = '%1.2f%%',  startangle = 90 )\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#veriönişleme\n",
    "\n",
    "WPT = nltk.WordPunctTokenizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('turkish')\n",
    "\n",
    "\n",
    "docs = data['tweets']\n",
    "docs = docs.map(lambda x: re.sub('[,\\.!?();:$%&#\"]', '', x))\n",
    "docs = docs.map(lambda x: x.lower())\n",
    "docs = docs.map(lambda x: x.strip())\n",
    "\n",
    "#stopwordleri kaldırmak için\n",
    "def token(values):\n",
    "    filtered_words = [word for word in values.split() if word not in stop_word_list]\n",
    "    not_stopword_doc = \" \".join(filtered_words)\n",
    "    return not_stopword_doc\n",
    "\n",
    "docs = docs.map(lambda x: token(x))\n",
    "data['tweets'] = docs\n",
    "\n",
    "print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"sentiment\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDoc = data['tweets'].values.tolist()\n",
    "dataClass = data['sentiment'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test ve train olarak ayırma \n",
    "x_train, x_test, y_train, y_test = train_test_split(dataDoc, dataClass, test_size = 0.2, random_state = 42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5)\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "#tfidf \n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df = 5)\n",
    "\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OneVsRest LogisticRegression modeli¶\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#logistic regresyon\n",
    "model = OneVsRestClassifier(LogisticRegression(penalty = 'l2', C=1.0))\n",
    "model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "print (\"Logistic Regression Accuracy={}\".format(accuracy_score(y_test, model.predict(x_test_tfidf))))\n",
    "logisticpred = accuracy_score(y_test, model.predict(x_test_tfidf))\n",
    "\n",
    "\n",
    "#stochastic gradient descent\n",
    "model2 = OneVsRestClassifier(SGDClassifier(loss = 'hinge', penalty = 'elasticnet', max_iter = 5))\n",
    "model2.fit(x_train_tfidf, y_train)\n",
    "print (\"SGD Accuracy={}\".format(accuracy_score(y_test, model2.predict(x_test_tfidf))))\n",
    "sgdpred = accuracy_score(y_test, model2.predict(x_test_tfidf))\n",
    "\n",
    "\n",
    "#lineersvc\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "SVC_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "SVC_pipeline.fit(x_train_tfidf, y_train)\n",
    "\n",
    "prediction = SVC_pipeline.predict(x_test_tfidf)\n",
    "print('LineerSVC accuracy is {}'.format(accuracy_score(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nelerin konuşulduğunun analizi için\n",
    "import pandas as pd\n",
    "df=pd.read_excel(r'C:\\Users\\ASUS\\Desktop\\kategoriketiketler.xlsx')\n",
    "df.head()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "max=80\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Tweets'], df['Labels'], test_size=0.20,random_state = 8)\n",
    "count_vect = CountVectorizer( binary=True)\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB(alpha=0.1,fit_prior=False).fit(X_train_tfidf, y_train)\n",
    "clf1=SVC().fit(X_train_tfidf, y_train)\n",
    "test_features = count_vect.transform(X_test)\n",
    "y_pred=clf.predict(test_features)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "target_names=['Siyaset','Sağlık','Dünya','Spor'] \n",
    "y_pred=clf.predict_proba(test_features)*100\n",
    "pred_df=pd.DataFrame(y_pred,columns=target_names)\n",
    "\n",
    "pred_df=pd.DataFrame(y_pred,columns=target_names)\n",
    "print(pred_df.iloc[231,:])\n",
    "\n",
    "\n",
    "#%40 olasılık değerinden yukarıda olan tweetleri getirir.  \n",
    "sayac=0\n",
    "max=0\n",
    "col=''\n",
    "pred_df=pd.DataFrame(y_pred,columns=target_names)\n",
    "for i in range(len(pred_df)):\n",
    "    for j in pred_df.columns:\n",
    "        \n",
    "        if pred_df.loc[i,j]>40:\n",
    "            sayac=sayac+1    #threshold değeri \n",
    "            print(pred_df.loc[i,j])\n",
    "            print(i)\n",
    "            print(j)\n",
    "            print(pred_df.iloc[i,:])\n",
    "\n",
    "    sayac=0\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_excel(r\"C:\\Users\\ASUS\\Desktop\\covid19tweetler.xlsx\")\n",
    "test_data_list=[]\n",
    "for i in range(len(test_data)):\n",
    "    test_data_list.append(test_data.iloc[i,0])\n",
    "    \n",
    "test_etiketsiz = count_vect.transform(test_data_list)\n",
    "y_pred_etiketsiz=clf.predict_proba(test_etiketsiz)*100\n",
    "\n",
    "pred_df_test=pd.DataFrame(y_pred_etiketsiz,columns=target_names)\n",
    "print(pred_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yeni gözlem\n",
    "\n",
    "sayac=0\n",
    "max=0\n",
    "ilk=0\n",
    "ilkin_etiketi=''\n",
    "col=''\n",
    "toplam=0\n",
    "xc=pd.DataFrame(test_data)\n",
    "pred_df_test=pd.DataFrame(y_pred_etiketsiz,columns=target_names)\n",
    "tweet=[]\n",
    "\n",
    "def test_tweet(i,j,deger):\n",
    "    #print(\"Tweets:\",xc.iloc[i,0])\n",
    "    tweet.append((xc.iloc[i,0],j,deger))\n",
    "\n",
    "for i in range(len(xc)):\n",
    "    for j in pred_df_test.columns:        \n",
    "        if pred_df_test.loc[i,j]>40: #threshold değeri \n",
    "            sayac=sayac+1   \n",
    "            if sayac==1:\n",
    "                ilk=pred_df_test.loc[i,j]\n",
    "                ilkin_etiketi=j\n",
    "                toplam=toplam+1\n",
    "                \n",
    "                test_tweet(i,ilkin_etiketi,ilk)               \n",
    "                #print(\"************************************************\")\n",
    "            if sayac>=2:                \n",
    "                #print(\"İkincisinin Değeri:\",pred_df_test.loc[i,j])\n",
    "                #print(\"İkincisinin Etiketi\",j)\n",
    "                toplam=toplam+1\n",
    "                test_tweet(i,j,pred_df_test.loc[i,j])\n",
    "                #print(\"************************************************\")    \n",
    "\n",
    "    sayac=0\n",
    "\n",
    "tweet=pd.DataFrame(tweet).to_excel(r'C:\\Users\\ASUS\\Desktop\\TekveÇiftetiketeatanantwitler.xlsx',encoding=\"latin-1\")\n",
    "print(\"Toplam:\",toplam)  \n",
    "#18100 adet tweetten 17726 tanesi en az 1 tane sınıfa gitmiştir.17726 tanesinin içinden de 1028 tane tweet ise çift etikete gitmiştir.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
